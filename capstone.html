<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Capstone Project</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header class="project_header">
        <h1 class="project_name">Data Science Capstone Project: [Project Title]</h1>

    </header>

    <main>
        <section id="overview">
            <h2>Introduction</h2>
                <div class="introduction">
                    <p> Neural networks, mirroring the human brainâ€™s pattern recognition abilities, 
                        have revolutionized machine learning. Their success spans various fields, 
                        from healthcare to finance, showcasing exceptional problem-solving and data interpretation capabilities. 
                        As neural networks increasingly outperform traditional algorithms, it becomes crucial to unravel their complex learning mechanisms. 
                        This understanding not only advances our knowledge but also opens doors to more robust and efficient applications. 
                        Our project aims to delve into these intricacies, offering deeper insights into the functioning of neural networks and their 
                        evolving role in modern technology.
                    </p>
                     <p>The study of feature evolution throughout the training phase in neural networks is pivotal in advancing our 
                        understanding of modern machine learning methodologies. The previous studys Beaglehole et al. (2023) and Radhakrishnan et al. (2023) formulated Convolutional
                        Neural Feature Ansatz which the features selected by convolutional networks can be recovered by computing the average gradient outer product (AGOP) of the trained network with 
                        respect to image patches given by empirical covariance matrices of filters at any given layer. 
                    </p>
                    <p>In this paper, we are going to concentrates on examining the progression of image data
                        features within convolutional networks, specifically through the lens of training ResNet-18 and LeNet models on the MNIST dataset. Our investigation delves into the selection process 
                        of features by individual filters and their increasing significance in capturing the intrinsic patterns present in the data. By elucidating the path of feature evolution in neural networks, 
                        our aim is to shed light on their learning dynamics, potentially paving the way for enhanced performance in tackling more sophisticated tasks.
                    </p>
                </div>
            
            
        </section>

        <section id="Method">
            <h2>Method and experiment</h2>
            <div class="method">
                <p>In this project, we are going to use the MNIST (Modified National Institute of Standards and Technology database). The MNIST dataset is a large database consisting of handwritten digits ranging from 0 to 9, where each image is a 28x28 pixels in size.
                    Moving forward, possible datasets to investigate includes ImageNet and CIFAR-10...
                </p>
                <p>We used a VGG11 model with 8 convolutional layers. We experimented on the MNIST dataset and extracted the 
                    values W T W and average gradient outer product (AGOP) in each layer. We looked at the correlations between the initial & trained 
                    CNFM and the trained CNFM & AGOP. Our investigation suggests that there is a high correlation between the trained CNFM and AGOP as 
                    shown on Table 1.
                </p>

                <center>
                    <figure>
                    <img src="table1.png" alt="Trulli" style="width:100%" />
                    <figcaption align="center"><b>Table 1: Correlation between Initial & Trained CRFM and Trained CNFM & AGOP across Layers</b></figcaption>
                    </figure>
                </center>

                <p>
                    We have also plotted the first layer of CNFM and AGOP to provide visual interpretability and insight into the correlation between these two.
                </p>

                <center>
                    <figure>
                    <img src="figure1.jpg" alt="Trulli" style="width:100%" />
                    <figcaption align="center"><b>Figure 1: Layer 1 CNFM and AGOP</b></figcaption>
                    </figure>
                </center>
            </div>
        </section>

        <section id="Result">
            <h2>Result</h2>
            <div class="result">
                <center>
                    <figure>
                    <img src="figure2.jpg" alt="Trulli" style="width:100%" />
                    <figcaption align="center"><b>Figure 2: Transform Image with Neural Network Matrix</b></figcaption>
                    </figure>
                </center>
    
                <center>
                    <figure>
                    <img src="figure3.jpg" alt="Trulli" style="width:100%" />
                    <figcaption align="center"><b>Figure 3: Transform Image with AGOP</b></figcaption>
                    </figure>
                </center>
            </div>
        </section>

    </main>

    <footer>
        <!-- Add footer content here -->
    </footer>
</body>
</html>